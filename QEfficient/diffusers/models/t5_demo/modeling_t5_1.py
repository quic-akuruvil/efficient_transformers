from transformers.models.t5.modeling_t5 import T5Config, T5Model, T5PreTrainedModel, T5LayerNorm, T5EncoderModel
# import torch as nn
# import torch
# import math


# class T5LayerNorm(nn.Module):
#     def __init__(self, hidden_size, eps=1e-6):
#         """
#         Construct a layernorm module in the T5 style. No bias and no subtraction of mean.
#         """
#         super().__init__()
#         self.weight = nn.Parameter(torch.ones(hidden_size))
#         self.variance_epsilon = eps

#     def forward(self, hidden_states):
#         # T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean
#         # Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated
#         # w/o mean and there is no bias. Additionally we want to make sure that the accumulation for
#         # half-precision inputs is done in fp32

#         div_first = hidden_states * torch.rsqrt(torch.tensor(hidden_states.shape[-1], dtype=torch.float32))
#         variance = div_first.pow(2).sum(-1, keepdim=True)
#         hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)

#         # convert into half-precision if necessary
#         if self.weight.dtype in [torch.float16, torch.bfloat16]:
#             hidden_states = hidden_states.to(self.weight.dtype)

#         return self.weight * hidden_states

# class T5LayerFF():
#     def __init__(self, config: T5Config):
#         super().__init__()
#         if config.is_gated_act:
#             self.DenseReluDense = T5DenseGatedActDense(config)
#         else:
#             self.DenseReluDense = T5DenseActDense(config)

#         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
#         self.dropout = nn.Dropout(config.dropout_rate)
#         self.scaling_factor = nn.Parameter(torch.tensor(1.0))

#     def forward(self, hidden_states):
#         forwarded_states = self.layer_norm(hidden_states)
#         forwarded_states = self.DenseReluDense(forwarded_states)
#         hidden_states = hidden_states * self.scaling_factor + self.dropout(forwarded_states)
#         return hidden_states

# class T5Attention(nn.Module):
#     def __init__(self, config: T5Config, has_relative_attention_bias=False):
#         super().__init__()
#         self.is_decoder = config.is_decoder
#         self.has_relative_attention_bias = has_relative_attention_bias
#         self.relative_attention_num_buckets = config.relative_attention_num_buckets
#         self.relative_attention_max_distance = config.relative_attention_max_distance
#         self.d_model = config.d_model
#         self.key_value_proj_dim = config.d_kv
#         self.n_heads = config.num_heads
#         self.dropout = config.dropout_rate
#         self.inner_dim = self.n_heads * self.key_value_proj_dim

#         # Mesh TensorFlow initialization to avoid scaling before softmax
#         self.q = nn.Linear(self.d_model, self.inner_dim, bias=False)
#         self.k = nn.Linear(self.d_model, self.inner_dim, bias=False)
#         self.v = nn.Linear(self.d_model, self.inner_dim, bias=False)
#         self.o = nn.Linear(self.inner_dim, self.d_model, bias=False)

#         if self.has_relative_attention_bias:
#             self.relative_attention_bias = nn.Embedding(self.relative_attention_num_buckets, self.n_heads)
#         self.pruned_heads = set()
#         self.gradient_checkpointing = False

#     @staticmethod
#     def _relative_position_bucket(relative_position, bidirectional=True, num_buckets=32, max_distance=128):
#         """
#         Adapted from Mesh Tensorflow:
#         https://github.com/tensorflow/mesh/blob/0cb87fe07da627bf0b7e60475d59f95ed6b5be3d/mesh_tensorflow/transformer/transformer_layers.py#L593

#         Translate relative position to a bucket number for relative attention. The relative position is defined as
#         memory_position - query_position, i.e. the distance in tokens from the attending position to the attended-to
#         position. If bidirectional=False, then positive relative positions are invalid. We use smaller buckets for
#         small absolute relative_position and larger buckets for larger absolute relative_positions. All relative
#         positions >=max_distance map to the same bucket. All relative positions <=-max_distance map to the same bucket.
#         This should allow for more graceful generalization to longer sequences than the model has been trained on

#         Args:
#             relative_position: an int32 Tensor
#             bidirectional: a boolean - whether the attention is bidirectional
#             num_buckets: an integer
#             max_distance: an integer

#         Returns:
#             a Tensor with the same shape as relative_position, containing int32 values in the range [0, num_buckets)
#         """
#         relative_buckets = 0
#         if bidirectional:
#             num_buckets //= 2
#             relative_buckets += (relative_position > 0).to(torch.long) * num_buckets
#             relative_position = torch.abs(relative_position)
#         else:
#             relative_position = -torch.min(relative_position, torch.zeros_like(relative_position))
#         # now relative_position is in the range [0, inf)

#         # half of the buckets are for exact increments in positions
#         max_exact = num_buckets // 2
#         is_small = relative_position < max_exact

#         # The other half of the buckets are for logarithmically bigger bins in positions up to max_distance
#         relative_position_if_large = max_exact + (
#             torch.log(relative_position.float() / max_exact)
#             / math.log(max_distance / max_exact)
#             * (num_buckets - max_exact)
#         ).to(torch.long)
#         relative_position_if_large = torch.min(
#             relative_position_if_large, torch.full_like(relative_position_if_large, num_buckets - 1)
#         )

#         relative_buckets += torch.where(is_small, relative_position, relative_position_if_large)
#         return relative_buckets

#     def compute_bias(self, query_length, key_length, device=None):
#         """Compute binned relative position bias"""
#         if device is None:
#             device = self.relative_attention_bias.weight.device
#         context_position = torch.arange(query_length, dtype=torch.long, device=device)[:, None]
#         memory_position = torch.arange(key_length, dtype=torch.long, device=device)[None, :]
#         relative_position = memory_position - context_position  # shape (query_length, key_length)
#         relative_position_bucket = self._relative_position_bucket(
#             relative_position,  # shape (query_length, key_length)
#             bidirectional=(not self.is_decoder),
#             num_buckets=self.relative_attention_num_buckets,
#             max_distance=self.relative_attention_max_distance,
#         )
#         values = self.relative_attention_bias(relative_position_bucket)  # shape (query_length, key_length, num_heads)
#         values = values.permute([2, 0, 1]).unsqueeze(0)  # shape (1, num_heads, query_length, key_length)
#         return values

#     def forward(
#         self,
#         hidden_states,
#         mask=None,
#         key_value_states=None,
#         position_bias=None,
#         past_key_value=None,
#         layer_head_mask=None,
#         query_length=None,
#         use_cache=False,
#         output_attentions=False,
#     ):
#         """
#         Self-attention (if key_value_states is None) or attention over source sentence (provided by key_value_states).
#         """
#         # Input is (batch_size, seq_length, dim)
#         # Mask is (batch_size, key_length) (non-causal) or (batch_size, key_length, key_length)
#         # past_key_value[0] is (batch_size, n_heads, q_len - 1, dim_per_head)
#         batch_size, seq_length = hidden_states.shape[:2]

#         real_seq_length = seq_length

#         if past_key_value is not None:
#             if len(past_key_value) != 2:
#                 raise ValueError(
#                     f"past_key_value should have 2 past states: keys and values. Got { len(past_key_value)} past states"
#                 )
#             real_seq_length += past_key_value[0].shape[2] if query_length is None else query_length

#         key_length = real_seq_length if key_value_states is None else key_value_states.shape[1]

#         def shape(states):
#             """projection"""
#             return states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)

#         def unshape(states):
#             """reshape"""
#             return states.transpose(1, 2).contiguous().view(batch_size, -1, self.inner_dim)

#         def project(hidden_states, proj_layer, key_value_states, past_key_value):
#             """projects hidden states correctly to key/query states"""
#             if key_value_states is None:
#                 # self-attn
#                 # (batch_size, n_heads, seq_length, dim_per_head)
#                 hidden_states = shape(proj_layer(hidden_states))
#             elif past_key_value is None:
#                 # cross-attn
#                 # (batch_size, n_heads, seq_length, dim_per_head)
#                 hidden_states = shape(proj_layer(key_value_states))

#             if past_key_value is not None:
#                 if key_value_states is None:
#                     # self-attn
#                     # (batch_size, n_heads, key_length, dim_per_head)
#                     hidden_states = torch.cat([past_key_value, hidden_states], dim=2)
#                 elif past_key_value.shape[2] != key_value_states.shape[1]:
#                     # checking that the `sequence_length` of the `past_key_value` is the same as
#                     # the provided `key_value_states` to support prefix tuning
#                     # cross-attn
#                     # (batch_size, n_heads, seq_length, dim_per_head)
#                     hidden_states = shape(proj_layer(key_value_states))
#                 else:
#                     # cross-attn
#                     hidden_states = past_key_value
#             return hidden_states

#         # get query states
#         query_states = shape(self.q(hidden_states))  # (batch_size, n_heads, seq_length, dim_per_head)

#         # get key/value states
#         key_states = project(
#             hidden_states, self.k, key_value_states, past_key_value[0] if past_key_value is not None else None
#         )
#         value_states = project(
#             hidden_states, self.v, key_value_states, past_key_value[1] if past_key_value is not None else None
#         )

#         # compute scores
#         scores = torch.matmul(
#             query_states, key_states.transpose(3, 2)
#         )  # equivalent of torch.einsum("bnqd,bnkd->bnqk", query_states, key_states), compatible with onnx op>9

#         if position_bias is None:
#             if not self.has_relative_attention_bias:
#                 position_bias = torch.zeros(
#                     (1, self.n_heads, real_seq_length, key_length), device=scores.device, dtype=scores.dtype
#                 )
#                 if self.gradient_checkpointing and self.training:
#                     position_bias.requires_grad = True
#             else:
#                 position_bias = self.compute_bias(real_seq_length, key_length, device=scores.device)

#             # if key and values are already calculated
#             # we want only the last query position bias
#             if past_key_value is not None:
#                 #position_bias = position_bias[:, :, -hidden_states.size(1) :, :]
#                 position_bias = position_bias[:, :, -1:, :]

#             if mask is not None:
#                 position_bias = position_bias + mask  # (batch_size, n_heads, seq_length, key_length)

#         if self.pruned_heads:
#             mask = torch.ones(position_bias.shape[1])
#             mask[list(self.pruned_heads)] = 0
#             position_bias_masked = position_bias[:, mask.bool()]
#         else:
#             position_bias_masked = position_bias

#         scores += position_bias_masked
#         attn_weights = nn.functional.softmax(scores.float(), dim=-1).type_as(
#             scores
#         )  # (batch_size, n_heads, seq_length, key_length)
#         attn_weights = nn.functional.dropout(
#             attn_weights, p=self.dropout, training=self.training
#         )  # (batch_size, n_heads, seq_length, key_length)

#         # Mask heads if we want to
#         if layer_head_mask is not None:
#             attn_weights = attn_weights * layer_head_mask

#         attn_output = unshape(torch.matmul(attn_weights, value_states))  # (batch_size, seq_length, dim)
#         attn_output = self.o(attn_output)

#         present_key_value_state = (key_states, value_states) if (self.is_decoder and use_cache) else None
#         outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)

#         if output_attentions:
#             outputs = outputs + (attn_weights,)
#         return outputs


# class T5LayerSelfAttention(nn.Module):
#     def __init__(self, config, has_relative_attention_bias=False):
#         super().__init__()
#         self.SelfAttention = T5Attention(config, has_relative_attention_bias=has_relative_attention_bias)
#         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
#         self.dropout = nn.Dropout(config.dropout_rate)
#         self.scaling_factor = nn.Parameter(torch.tensor(1.0))

#     def forward(
#         self,
#         hidden_states,
#         attention_mask=None,
#         position_bias=None,
#         layer_head_mask=None,
#         past_key_value=None,
#         use_cache=False,
#         output_attentions=False,
#     ):
#         normed_hidden_states = self.layer_norm(hidden_states)
#         attention_output = self.SelfAttention(
#             normed_hidden_states,
#             mask=attention_mask,
#             position_bias=position_bias,
#             layer_head_mask=layer_head_mask,
#             past_key_value=past_key_value,
#             use_cache=use_cache,
#             output_attentions=output_attentions,
#         )
#         hidden_states = hidden_states * self.scaling_factor + self.dropout(attention_output[0])
#         outputs = (hidden_states,) + attention_output[1:]  # add attentions if we output them
#         return outputs


# class T5LayerCrossAttention(nn.Module):
#     def __init__(self, config):
#         super().__init__()
#         self.EncDecAttention = T5Attention(config, has_relative_attention_bias=False)
#         self.layer_norm = T5LayerNorm(config.d_model, eps=config.layer_norm_epsilon)
#         self.dropout = nn.Dropout(config.dropout_rate)
#         self.scaling_factor = nn.Parameter(torch.tensor(1.0))

#     def forward(
#         self,
#         hidden_states,
#         key_value_states,
#         attention_mask=None,
#         position_bias=None,
#         layer_head_mask=None,
#         past_key_value=None,
#         use_cache=False,
#         query_length=None,
#         output_attentions=False,
#     ):
#         normed_hidden_states = self.layer_norm(hidden_states)
#         attention_output = self.EncDecAttention(
#             normed_hidden_states,
#             mask=attention_mask,
#             key_value_states=key_value_states,
#             position_bias=position_bias,
#             layer_head_mask=layer_head_mask,
#             past_key_value=past_key_value,
#             use_cache=use_cache,
#             query_length=query_length,
#             output_attentions=output_attentions,
#         )
#         layer_output = hidden_states * self.scaling_factor + self.dropout(attention_output[0])
#         outputs = (layer_output,) + attention_output[1:]  # add attentions if we output them
#         return outputs